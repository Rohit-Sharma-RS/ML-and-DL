{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu transformers sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltbtkgB6-OUn",
        "outputId": "fc00f544-a54d-4cee-cecc-774a35c08f77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu, sentence-transformers\n",
            "Successfully installed faiss-cpu-1.8.0.post1 sentence-transformers-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TEwaxh-C9o2C"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load a pre-trained model for embeddings and a transformer model for generation\n",
        "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  # Embeddings for retrieval\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')  # Generative model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample corpus to retrieve from\n",
        "documents = [\n",
        "    \"The Eiffel Tower is in Paris.\",\n",
        "    \"The Mona Lisa is a painting by Leonardo da Vinci.\",\n",
        "    \"The Great Wall of China is visible from space.\",\n",
        "    \"Albert Einstein developed the theory of relativity.\",\n",
        "    \"Python is the most popular programming language in the U.S.\",\n",
        "]"
      ],
      "metadata": {
        "id": "U_SIxKcDA9Ow"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Encode documents into embeddings for FAISS\n",
        "doc_embeddings = embedder.encode(documents, convert_to_tensor=True).cpu().detach().numpy()\n",
        "\n",
        "# 2. Initialize FAISS index\n",
        "index = faiss.IndexFlatL2(doc_embeddings.shape[1])  # L2 similarity\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "def retrieve(query, top_k=1):\n",
        "    # 3. Get the query embedding and perform the search\n",
        "    query_embedding = embedder.encode([query], convert_to_tensor=True).cpu().detach().numpy()\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Retrieve top K documents\n",
        "    retrieved_docs = [documents[i] for i in indices[0]]\n",
        "    return retrieved_docs"
      ],
      "metadata": {
        "id": "n8Vv7KSlBCeq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def generate_answer(query):\n",
        "    # # 4. Use the retrieval step\n",
        "    # retrieved_docs = retrieve(query, top_k=1)  # Limit to top 1 document\n",
        "    # context = \" \".join(retrieved_docs)\n",
        "\n",
        "    # # 5. Generate response using a generative model\n",
        "    # input_text = f\"Context: {context} Question: {query}\"\n",
        "    # inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    # outputs = model.generate(**inputs)\n",
        "\n",
        "    # # Decode the output to get the generated text\n",
        "    # answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # return answer"
      ],
      "metadata": {
        "id": "SXkenXh8BDaF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "  # 1. Use the retrieval step\n",
        "  retrieved_docs = retrieve(query, top_k=1)  # Limit to top 1 document\n",
        "  context = \" \".join(retrieved_docs)\n",
        "\n",
        "  # 2. Set up the input text for the model\n",
        "  input_text = f\"Context: {context} Question: {query}\"\n",
        "\n",
        "  # 3. Tokenize the input, specifying max_length and truncation\n",
        "  inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "  # 4. Generate response using the model\n",
        "  outputs = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Decode the output to get the generated text\n",
        "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "dZoOEa0EBHJB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"Which is the most popular programming language?\"\n",
        "answer = generate_answer(query)\n",
        "print(f\"Question: {query}\")\n",
        "print(f\"Answer: {answer.split('. ')[0]}\") # for one line answers :)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWWAQ8iCBM6A",
        "outputId": "f4364a32-4bef-420d-ebcd-066408fe9481"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Which is the most popular programming language?\n",
            "Answer: Python is the most popular programming language in the U.S\n"
          ]
        }
      ]
    }
  ]
}