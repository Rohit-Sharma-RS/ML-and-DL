{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1987ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from utils.items import Item\n",
    "from utils.loaders import ItemLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import  numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN = \"\\033[92m\"\n",
    "RED = \"\\033[91m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RESET= \"\\033[0m\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a214b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# login to huggingface hub\n",
    "login(os.getenv(\"HF_TOKEN\"), add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eaf33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0].prompt)\n",
    "train[0].price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    test[i].prompt = test[i].prompt.split(\"Price is \")[0] + \"Price is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].details # a string not a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc239cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train:\n",
    "    item.features = json.loads(item.details)\n",
    "for item in test:\n",
    "    item.features = json.loads(item.details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can easily find the most common features in the dataset\n",
    "feature_count = Counter()\n",
    "for item in train:\n",
    "    for f in item.features.keys():\n",
    "        feature_count[f] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71199ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79932f8b",
   "metadata": {},
   "source": [
    "## Now time for some Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(train[i].features['Item Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638940f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different weights for different columns!!\n",
    "\n",
    "def get_weight(item):\n",
    "    weight_str = item.features.get('Item Weight')\n",
    "    if weight_str:\n",
    "        parts = weight_str.split(' ')\n",
    "        amount = float(parts[0])\n",
    "        unit = parts[1].lower()\n",
    "        if unit==\"pounds\":\n",
    "            return amount\n",
    "        elif unit==\"ounces\":\n",
    "            return amount / 16\n",
    "        elif unit==\"grams\":\n",
    "            return amount / 453.592\n",
    "        elif unit==\"milligrams\":\n",
    "            return amount / 453592\n",
    "        elif unit==\"kilograms\":\n",
    "            return amount / 0.453592\n",
    "        elif unit==\"hundredths\" and parts[2].lower()==\"pounds\":\n",
    "            return amount / 100\n",
    "        else:\n",
    "            print(weight_str)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84debbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [get_weight(item) for item in train]\n",
    "weights = [w for w in weights if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weight = sum(weights) / len(weights)\n",
    "average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2273d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_with_default(item):\n",
    "    weight = get_weight(item)\n",
    "    return weight or average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e254c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(item):\n",
    "    rank_dict = item.features.get('Best Sellers Rank')\n",
    "    if rank_dict:\n",
    "        ranks = rank_dict.values()\n",
    "        return sum(ranks)/len(ranks)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7831314",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [get_rank(item) for item in train]\n",
    "ranks = [r for r in ranks if r]\n",
    "average_rank = sum(ranks) / len(ranks)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_with_default(item):\n",
    "    rank = get_rank(item)\n",
    "    return rank or average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d82ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(item):\n",
    "    return len(item.test_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = Counter()\n",
    "\n",
    "for t in train:\n",
    "    brand = t.features.get('Brand')\n",
    "    if brand:\n",
    "        brands[brand] += 1\n",
    "\n",
    "brands.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9631362",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_BRANDS = [b[0] for b in brands.most_common(20)]\n",
    "\n",
    "def is_top_brand(item):\n",
    "    brand = item.features.get('Brand')\n",
    "    return brand in TOP_BRANDS if brand else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56982a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(item):\n",
    "    return {\n",
    "        \"weight\": get_weight_with_default(item),\n",
    "        \"rank\": get_rank_with_default(item),\n",
    "        \"text_length\": get_text_length(item),\n",
    "        \"top_brand\": is_top_brand(item)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b007446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dataframe(items):\n",
    "        features = [get_features(item) for item in items]\n",
    "        df = pd.DataFrame(features)\n",
    "        df['price'] = [item.price for item in items]\n",
    "        return df\n",
    "\n",
    "train_df = list_to_dataframe(train)\n",
    "test_df = list_to_dataframe(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f340354",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "feature_columns = ['weight', 'rank', 'text_length', 'top_brand']\n",
    "\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['price']\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['price']\n",
    "\n",
    "# Train a Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "for feature, coef in zip(feature_columns, model.coef_):\n",
    "    print(f\"{feature}: {coef}\") # how much each \n",
    "                                #feature contributes to the price\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# Predict the test set and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305495a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict price for a new item\n",
    "\n",
    "def linear_regression_pricer(item):\n",
    "    features = get_features(item)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    return model.predict(features_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = np.array([float(item.price) for item in train])\n",
    "documents = [item.test_prompt() for item in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4284a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the count vectorizer to create a bag of words representation\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a62aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "mse = mean_squared_error(prices, regressor.predict(X))\n",
    "r2 = r2_score(prices, regressor.predict(X))\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "print(f\"Intercept: {regressor.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011058c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(prices, regressor.predict(X), alpha=0.5)\n",
    "plt.plot([min(prices), max(prices)], [min(prices), max(prices)], color='red', lw=2)\n",
    "plt.xlabel('True Prices')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab638e",
   "metadata": {},
   "source": [
    "## Using Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "processed_docs = [simple_preprocess(doc) for doc in documents]\n",
    "w2v_model = Word2Vec(processed_docs, vector_size=400, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    doc_words = simple_preprocess(doc)\n",
    "    word_vectors = [w2v_model.wv[word] for word in doc_words if word in w2v_model.wv]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(w2v_model.vector_size)\n",
    "    \n",
    "X_w2v = np.array([document_vector(doc) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fea6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_lr = LinearRegression()\n",
    "word2vec_lr.fit(X_w2v, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "mse = mean_squared_error(prices, regressor.predict(X))\n",
    "r2 = r2_score(prices, regressor.predict(X))\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "print(f\"Intercept: {regressor.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize the predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(prices, word2vec_lr.predict(X_w2v), alpha=0.5)\n",
    "plt.plot([min(prices), max(prices)], [min(prices), max(prices)], color='red', lw=2)\n",
    "plt.xlabel('True Prices')\n",
    "plt.ylabel('Predicted Prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f45279",
   "metadata": {},
   "source": [
    "## Time to Use SVR and Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84eabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# X_reduced = pca.fit_transform(X_w2v)\n",
    "# svr_regressor = SVR()\n",
    "\n",
    "# svr_regressor.fit(X_reduced, prices)\n",
    "# # took a million years to run SVR trying dimensional reduction\n",
    "\n",
    "# Use LinearSVR for regression no kernels will ever work!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde266d0",
   "metadata": {},
   "source": [
    "### I tried LightGBM and XGBoost too and they are wonderful got deleted, however same as Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # metrics\n",
    "# mse = mean_squared_error(prices, regressor.predict(X))\n",
    "# r2 = r2_score(prices, regressor.predict(X))\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "# print(f\"R-squared Score: {r2}\")\n",
    "# print(f\"Intercept: {regressor.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41aafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize the predictions\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(prices, svr_regressor.predict(X_reduced), alpha=0.5)\n",
    "# plt.plot([min(prices), max(prices)], [min(prices), max(prices)], color='red', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f563496",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_w2v, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed585ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "mse = mean_squared_error(prices, rf_model.predict(X_w2v))\n",
    "r2 = r2_score(prices, rf_model.predict(X_w2v))\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "print(f\"Intercept: {regressor.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(prices, rf_model.predict(X_w2v), alpha=0.5)\n",
    "plt.plot([min(prices), max(prices)], [min(prices), max(prices)], color='red', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be4a3a",
   "metadata": {},
   "source": [
    "## Ok now let's try a better way to visualize!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a841204",
   "metadata": {},
   "source": [
    "## Unveiling a mighty script that we will use a lot!\n",
    "\n",
    "A rather pleasing Test Harness that will evaluate any model against 250 items from the Test set\n",
    "\n",
    "And show us the results in a visually satisfying way.\n",
    "\n",
    "You write a function of this form:\n",
    "\n",
    "```\n",
    "def my_prediction_function(item):\n",
    "    # my code here\n",
    "    return my_estimate\n",
    "```\n",
    "\n",
    "And then you call:\n",
    "\n",
    "`Tester.test(my_prediction_function)`\n",
    "\n",
    "To evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Tester:\n",
    "\n",
    "    def __init__(self, predictor, title=None, data=test, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "    \n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint)\n",
    "        truth = datapoint.price\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint.title if len(datapoint.title) <= 40 else datapoint.title[:40]+\"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function):\n",
    "        cls(function).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60934575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_lr_pricer(item):\n",
    "    x = vectorizer.transform([item.test_prompt()])\n",
    "    return max(regressor.predict(x)[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198118a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_pricer(item):\n",
    "    doc = item.test_prompt()\n",
    "    doc_vector = document_vector(doc)\n",
    "    return max(0, rf_model.predict([doc_vector])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd54e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(bow_lr_pricer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
