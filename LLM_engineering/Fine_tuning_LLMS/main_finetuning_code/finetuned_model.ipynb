{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGc_1iLThwA8"
   },
   "outputs": [],
   "source": [
    "# pip installs\n",
    "\n",
    "!pip install -q datasets peft requests torch bitsandbytes transformers trl accelerate sentencepiece matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "711iVgYqhwqu"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from datetime import datetime\n",
    "from peft import PeftModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31uObfgogsfN"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "PROJECT_NAME = \"pricer\"\n",
    "HF_USER = \"Rohit-9898\"\n",
    "\n",
    "# encodings_to_try = ['latin1', 'utf-8', 'cp1252', 'iso-8859-1']\n",
    "\n",
    "# train_loaded = False\n",
    "# for encoding in encodings_to_try:\n",
    "#     try:\n",
    "#         with open(\"train.pkl\", \"rb\") as f:\n",
    "#             train = pickle.load(f, encoding=encoding)\n",
    "#         train_loaded = True\n",
    "#         print(f\"Successfully loaded 'train.pkl' using encoding: {encoding}\")\n",
    "#         break  # Stop if successful\n",
    "#     except UnicodeDecodeError as e:\n",
    "#         print(f\"Error loading 'train.pkl' with encoding {encoding}: {e}\")\n",
    "\n",
    "DATASET_NAME = \"rohitsharma77/lite-data\"\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "\n",
    "# Run name for saving the model in the hub\n",
    "\n",
    "\n",
    "RUN_NAME =  f\"2025-05-16_03.04.39\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "REVISION = None\n",
    "\n",
    "FINETUNED_MODEL = f\"{rohitsharma77/pricer-2025-05-06_04.12.09}\"\n",
    "\n",
    "# Hyperparameters for QLoRA\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "LORA_DROPOUT = 0.1\n",
    "QUANT_4_BIT = True # change this to quantize to 8 bits (larger)\n",
    "\n",
    "# Hyperparameters for Training\n",
    "\n",
    "EPOCHS = 1 # only 1 is needed more is probably overkill\n",
    "BATCH_SIZE = 1 # increase if more GPU power\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_SCHEDULER_TYPE = 'cosine'\n",
    "WARMUP_RATIO = 0.03\n",
    "OPTIMIZER = \"paged_adamw_32bit\"\n",
    "\n",
    "DATASET_NAME = \"rohitsharma77/pricer-data\"\n",
    "\n",
    "# note that SAVE_STEPS is how often it will upload to the hub\n",
    "# 2000 so that you get more frequent saves\n",
    "\n",
    "STEPS = 50\n",
    "SAVE_STEPS = 2000\n",
    "# LOG_TO_WANDB = True\n",
    "\n",
    "%matplotlib inline\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Used for writing to output in color\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU5N3RbvhtAK"
   },
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "from google.colab import userdata\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c05QOs0-h0Ni"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fnZVVJkie5x"
   },
   "outputs": [],
   "source": [
    "# pick the right quantization (thank you Robert M. for spotting the bug with the 8 bit version!)\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "20902ec2d6db464384b66a57aecf6df5",
      "dc204d27e76d44febddc5d6a6dbe0b45",
      "813e2ba6c0e84655b6cf2394d5e7d5ce",
      "0059408e3bee4f8988e156b470aeb193",
      "50cf79e64ba642ccb95de3ef68f5d531",
      "ff4409c02aca4ac19ccb333953b71daa",
      "67fe92271e49460cb3c651ebc30cbb9b",
      "058d25b9680545b1841aee09ddbf81c9",
      "b0c241a4dbb44db28d77df7cb6e9915c",
      "49264049bc9142b8a7e2cf39b790e96f",
      "116e09dbfdf04370bbd903e787a818b9"
     ]
    },
    "executionInfo": {
     "elapsed": 84918,
     "status": "ok",
     "timestamp": 1746557839869,
     "user": {
      "displayName": "Rohit Sharma",
      "userId": "00227136684466199517"
     },
     "user_tz": -330
    },
    "id": "HWs0qJTqifFH",
    "outputId": "9027d036-9a6d-467d-c6c5-219c99476181"
   },
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load the fine-tuned model with PEFT\n",
    "if REVISION:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL, revision=REVISION)\n",
    "else:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
    "\n",
    "\n",
    "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1746557839925,
     "user": {
      "displayName": "Rohit Sharma",
      "userId": "00227136684466199517"
     },
     "user_tz": -330
    },
    "id": "ZDdpr-pXihEB",
    "outputId": "2f25a264-0067-4f2f-9c01-d18228032c98"
   },
   "outputs": [],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTlaQZU9iiwi"
   },
   "outputs": [],
   "source": [
    "def extract_price(s):\n",
    "    if \"Price is $\" in s:\n",
    "      contents = s.split(\"Price is $\")[1]\n",
    "      contents = contents.replace(',','')\n",
    "      match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
    "      return float(match.group()) if match else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1746557839981,
     "user": {
      "displayName": "Rohit Sharma",
      "userId": "00227136684466199517"
     },
     "user_tz": -330
    },
    "id": "SwX5TPOYikMI",
    "outputId": "4c5f28c9-5a4a-4997-ed82-94c43af004c3"
   },
   "outputs": [],
   "source": [
    "extract_price(\"Price is $a fabulous 899.99 or so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLOkJbUmimCe"
   },
   "outputs": [],
   "source": [
    "# Original prediction function takes the most likely next token\n",
    "\n",
    "def model_predict(prompt):\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
    "    outputs = fine_tuned_model.generate(inputs, attention_mask=attention_mask, max_new_tokens=3, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    return extract_price(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkeENoEbin5M"
   },
   "outputs": [],
   "source": [
    "# An improved prediction function takes a weighted average of the top 3 choices\n",
    "\n",
    "top_K = 3\n",
    "\n",
    "def improved_model_predict(prompt, device=\"cuda\"):\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model(inputs, attention_mask=attention_mask)\n",
    "        next_token_logits = outputs.logits[:, -1, :].to('cpu')\n",
    "\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "    top_prob, top_token_id = next_token_probs.topk(top_K)\n",
    "    prices, weights = [], []\n",
    "    for i in range(top_K):\n",
    "      predicted_token = tokenizer.decode(top_token_id[0][i])\n",
    "      probability = top_prob[0][i]\n",
    "      try:\n",
    "        result = float(predicted_token)\n",
    "      except ValueError as e:\n",
    "        result = 0.0\n",
    "      if result > 0:\n",
    "        prices.append(result)\n",
    "        weights.append(probability)\n",
    "    if not prices:\n",
    "      return 0.0, 0.0\n",
    "    total = sum(weights)\n",
    "    weighted_prices = [price * weight / total for price, weight in zip(prices, weights)]\n",
    "    return sum(weighted_prices).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbo3mxkHipZm"
   },
   "outputs": [],
   "source": [
    "class Tester:\n",
    "\n",
    "    def __init__(self, predictor, data, title=None, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "\n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint[\"text\"])\n",
    "        truth = datapoint[\"price\"]\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function, data):\n",
    "        cls(function, data).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 358257,
     "status": "ok",
     "timestamp": 1746558198517,
     "user": {
      "displayName": "Rohit Sharma",
      "userId": "00227136684466199517"
     },
     "user_tz": -330
    },
    "id": "ubCawjygiq1z",
    "outputId": "70ade4d9-d0a8-4156-ba13-62887488ee26"
   },
   "outputs": [],
   "source": [
    "Tester.test(improved_model_predict, test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
