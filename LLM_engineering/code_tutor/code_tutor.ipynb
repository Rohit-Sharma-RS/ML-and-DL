{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0170a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945ab263",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_DEEPSEEK = 'deepseek-r1:1.5b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381b7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt=\"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt=\"Please give a detailed explanation to the following question: \" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed2abd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510cebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Explanation of the Code**\n",
       "\n",
       "This line of code is written in Python 3.4 and later versions, which introduced a feature called generators with `yield` expressions.\n",
       "\n",
       "The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` is a generator expression. It's similar to a list comprehension, but it uses the `yield from` keyword instead of explicit iteration.\n",
       "\n",
       "Let's break it down:\n",
       "\n",
       "- `{...}` : This denotes a set literal or a dictionary literal.\n",
       "- `book.get(\"author\")`: This gets the value associated with the key `\"author\"` in the `book` dictionary. If the key is not present, it returns `None`.\n",
       "- `for book in books if book.get(\"author\")`: This is a conditional iteration clause. It iterates over each item (`book`) in the `books` list, but only if the item has an `\"author\"` key with a non-`None` value.\n",
       "\n",
       "Now, let's talk about `yield from`. When we use `yield from`, it essentially \"passes through\" all items yielded by the expression following it. In other words, instead of yielding each individual item, it yields all the items that are already being yielded by the inner iterable (in this case, the generator expression).\n",
       "\n",
       "So, when we combine these two concepts, `yield from {book.get(\"author\") for book in books if book.get(\"author\")}`, what happens is:\n",
       "\n",
       "1. The generator expression `{book.get(\"author\") for book in books if book.get(\"author\")}` generates a sequence of values (e.g., `\"John Doe\"`, `\"Jane Smith\"`).\n",
       "2. `yield from` then passes each value generated by the inner iterable to an outer generator.\n",
       "\n",
       "**Why Does This Code Do What It Does?**\n",
       "\n",
       "This code is likely used in a data processing or scraping context, where we need to extract author names from a list of books. The `books` list contains dictionaries with book information, and we're filtering out any books that don't have an `\"author\"` key.\n",
       "\n",
       "By using this generator expression with `yield from`, we can:\n",
       "\n",
       "- Filter out books without authors\n",
       "- Extract the author name for each remaining book\n",
       "- Get a sequence of author names\n",
       "\n",
       "This is useful when working with large datasets or streaming data, as it avoids loading the entire dataset into memory at once. Instead, it processes one item at a time, which can be more memory-efficient.\n",
       "\n",
       "Here's an example of how you might use this code:\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"John Doe\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Jane Smith\"}\n",
       "]\n",
       "\n",
       "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(authors)  # Output: [\"John Doe\", \"Jane Smith\"]\n",
       "```\n",
       "In summary, this code uses a generator expression with `yield from` to filter out books without authors and extract the author names, making it a concise and memory-efficient way to process large datasets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "reply = response['message']['content']\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c4303",
   "metadata": {},
   "source": [
    "## Below Code streams Outputs and does it fast!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5606e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages():\n",
    "    question = input(\"Enter the coding question you have?\")\n",
    "\n",
    "    system_prompt=\"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "    user_prompt=\"Please give a detailed explanation to the following question: \" + question\n",
    "    \n",
    "    messages=[\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4491866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, I need to explain this code. Let me start by reading through the provided code and understanding each part step by step.\n",
       "\n",
       "First, there's a response variable initialized with 'ollama.chat('. Then, model is set to MODEL_DEEPSEEK. The messages function seems to be empty initially. Stream is set to True.\n",
       "\n",
       "Looking at display_handle, it uses Markdown to display full_response as it gets built from the response chunks. So, every time content is added from each chunk, if it's not empty, full_response is appended and displayed in markdown within the chunk.\n",
       "\n",
       "So, the code is setting up a response using Ollama's chat API with a specified model. The messages function is probably an empty list because there are no messages yet being sent or received. Stream=True means that any new content from the API will be streamed into a response variable, which is called full_response in this case.\n",
       "\n",
       "The display_handle updates its markdown view of full_response each time it receives new chunks. So every time Ollama sends back some data, it gets converted into markdown and displayed.\n",
       "\n",
       "I should explain what each line does clearly, maybe with an example if possible. Since I don't have a specific example right now, I'll describe the general behavior based on how Ollama typically works for text-to-speech conversion.\n",
       "</think>\n",
       "\n",
       "Here's a step-by-step explanation of the provided code:\n",
       "\n",
       "1. **Initialize Response:**\n",
       "   ```python\n",
       "   response = ollama.chat(model=MODEL_DEEPSEEK)\n",
       "   ```\n",
       "   - This line creates an instance using `ollama.chat()` with the specified model (`MODEL_DEEPSEEK`). The result is stored in the variable `response`.\n",
       "\n",
       "2. **Define Messages:**\n",
       "   ```python\n",
       "   messages = []\n",
       "   ```\n",
       "   - An empty list called `messages` is defined. It's likely used to store interactions or responses as chunks are sent back from Ollama.\n",
       "\n",
       "3. **Set Stream Mode:**\n",
       "   ```python\n",
       "   stream = True\n",
       "   ```\n",
       "   - The variable `stream` is set to `True`, indicating that any new content received by Ollama should be streamed into the response, not stored in `response`.\n",
       "\n",
       "4. **Initialize Display Handle:**\n",
       "   ```python\n",
       "   display_handle = display(Markdown(\"\"), display_id=True)\n",
       "   ```\n",
       "   - This line creates an instance of the `display` class with a Markdown content field that starts empty (` displays=\"\"`) and sets `display_id` to `True`. This setup is used to track and display any new text from Ollama.\n",
       "\n",
       "5. **Iterate Through Response Chunks:**\n",
       "   ```python\n",
       "   for chunk in response:\n",
       "       content = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
       "       if content:\n",
       "           full_response += content\n",
       "           display_handle.update(Markdown(full_response))\n",
       "   ```\n",
       "   - The loop iterates over each chunk of the response.\n",
       "     - `content = chunk.get(\"message\", {}).get(\"content\", \"\")`: For each message, it retrieves the \"content\" field. If not found (`content` is empty), it returns an empty string.\n",
       "     - `if content: full_response += content`: If the content exists, it adds this text to `full_response`.\n",
       "     - `display_handle.update(Markdown(full_response))`: This line updates the display handle with a markdown formatted version of `full_response`, which shows the processed text.\n",
       "\n",
       "**Summary:**\n",
       "The code is setting up an API call using Ollama with the model \"MODEL_DEEPSEEK\". It processes each chunk of the response, displays it in a markdown format within a display handle, and streams any new content directly into `full_response`.\n",
       "\n",
       "This setup is typical for converting text to speech using a machine learning model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "        model = MODEL_DEEPSEEK,\n",
    "        messages = messages(),\n",
    "        stream =True\n",
    "    )\n",
    "full_response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in response:\n",
    "    content = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "    if content:\n",
    "        full_response += content\n",
    "        display_handle.update(Markdown(full_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
