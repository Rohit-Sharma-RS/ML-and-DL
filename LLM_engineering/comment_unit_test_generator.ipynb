{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a735bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr\n",
    "from huggingface_hub import login, InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4311a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "\n",
    "# Initialize Hugging Face\n",
    "login(HF_TOKEN, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b81ae3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = {\n",
    "    \"Mixtral\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"Llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"DeepSeek\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    }\n",
    "\n",
    "# Define API endpoints\n",
    "MODEL_ENDPOINTS = {\n",
    "    \"Mixtral\": \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"Llama3\": \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0b4c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt_for_comment():\n",
    "    system = \"\"\"\n",
    "    You are a Python documentation expert. When writing documentation:\n",
    "    - Follow PEP 257 and Google docstring style guidelines\n",
    "    - Write clear, concise explanations\n",
    "    - Include practical examples\n",
    "    - Highlight edge cases and limitations\n",
    "    - Use type hints in docstrings\n",
    "    - Add inline comments only for complex logic\n",
    "    - Never skip documenting parameters or return values\n",
    "    - Validate that all documentation is accurate and complete\n",
    "    \"\"\"\n",
    "    return system\n",
    "\n",
    "def system_prompt_for_unit_test():\n",
    "    system = \"\"\"\n",
    "    You are an expert Python testing engineer who specializes in creating comprehensive unit tests. Follow these principles:\n",
    "    - Use pytest as the testing framework\n",
    "    - Follow the Arrange-Act-Assert pattern\n",
    "    - Test both valid and invalid inputs\n",
    "    - Include edge cases and boundary conditions\n",
    "    - Write descriptive test names that explain the scenario being tested\n",
    "    - Create independent tests that don't rely on each other\n",
    "    - Use appropriate fixtures and parametrize when needed\n",
    "    - Add clear comments explaining complex test logic\n",
    "    - Cover error cases and exceptions\n",
    "    - Achieve high code coverage while maintaining meaningful tests\n",
    "    \"\"\"\n",
    "    return system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cda11866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for_comment(code):\n",
    "    user = f\"\"\"\n",
    "    Please document this Python code with:\n",
    "    \n",
    "    1. A docstring containing:\n",
    "    - A clear description of purpose and functionality\n",
    "    - All parameters with types and descriptions\n",
    "    - Return values with types\n",
    "    - Exceptions that may be raised\n",
    "    - Any important notes or limitations\n",
    "    \n",
    "    2. Strategic inline comments for:\n",
    "    - Complex algorithms or business logic\n",
    "    - Non-obvious implementation choices\n",
    "    - Performance considerations\n",
    "    - Edge cases\n",
    "    \n",
    "    Here's the code to document:\n",
    "    \\n{code}\n",
    "    \"\"\"\n",
    "    return user\n",
    "\n",
    "def user_prompt_for_unit_test(code):\n",
    "    user = f\"\"\"\n",
    "    Please generate unit tests for the following Python code. Include:\n",
    "    \n",
    "    1. Test cases for:\n",
    "    - Normal/expected inputs\n",
    "    - Edge cases and boundary values\n",
    "    - Invalid inputs and error conditions\n",
    "    - Different combinations of parameters\n",
    "    - All public methods and functions\n",
    "    \n",
    "    2. For each test:\n",
    "    - Clear test function names describing the scenario\n",
    "    - Setup code (fixtures if needed)\n",
    "    - Test data preparation\n",
    "    - Expected outcomes\n",
    "    - Assertions checking results\n",
    "    - Comments explaining complex test logic\n",
    "    \n",
    "    3. Include any necessary:\n",
    "    - Imports\n",
    "    - Fixtures\n",
    "    - Mock objects\n",
    "    - Helper functions\n",
    "    - Test data generators\n",
    "    \n",
    "    Here's the code to test:\n",
    "    \\n{code}\n",
    "    \"\"\"\n",
    "    return user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e939980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for_comment(python, model_name):\n",
    "    system = system_prompt_for_comment()\n",
    "    user = user_prompt_for_comment(python)\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "\n",
    "def messages_for_unit_test(python, model_name):\n",
    "    system = system_prompt_for_unit_test()\n",
    "    user = user_prompt_for_unit_test(python)\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69c322",
   "metadata": {},
   "source": [
    "## Uncomment below code if you want to run on local device a model such as deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79d3b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# # Optional cache to avoid reloading model every time\n",
    "# _local_models = {}\n",
    "\n",
    "# def run_local_deepseek(prompt: str) -> str:\n",
    "#     model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "#     if \"DeepSeek\" not in _local_models:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#         model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).eval()\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         model.to(device)\n",
    "#         _local_models[\"DeepSeek\"] = (tokenizer, model, device)\n",
    "\n",
    "#     tokenizer, model, device = _local_models[\"DeepSeek\"]\n",
    "\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# def stream_with_hf_api(model_name, messages):\n",
    "#     \"\"\"Generate a response using Hugging Face's API or local inference for DeepSeek\"\"\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAMES[model_name])\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     if model_name == \"DeepSeek\":\n",
    "#         # Run locally\n",
    "#         output = run_local_deepseek(text)\n",
    "#         clean_output = re.sub(r'```(?:python)?\\n?|\\n?```', '', output.strip())\n",
    "#         yield clean_output\n",
    "#     else:\n",
    "#         # Use Hugging Face hosted model\n",
    "#         client = InferenceClient(MODEL_ENDPOINTS[model_name], token=HF_TOKEN)\n",
    "#         stream = client.text_generation(\n",
    "#             text,\n",
    "#             stream=True,\n",
    "#             details=True,\n",
    "#             max_new_tokens=3000,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.95\n",
    "#         )\n",
    "#         result = \"\"\n",
    "#         for r in stream:\n",
    "#             result += r.token.text\n",
    "#             yield re.sub(r'```(?:python)?\\n?|\\n?```', '', result.strip())\n",
    "\n",
    "# def comment_code(python, model):\n",
    "#     try:\n",
    "#         result = stream_with_hf_api(model, messages_for_comment(python, model))\n",
    "#         for stream_so_far in result:\n",
    "#             yield stream_so_far\n",
    "#     except Exception as e:\n",
    "#         yield f\"Error generating comments: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e30b0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_with_hf_api(model_name, messages):\n",
    "    \"\"\"Generate a streaming response using Hugging Face's API\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAMES[model_name])\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    client = InferenceClient(MODEL_ENDPOINTS[model_name], token=HF_TOKEN)\n",
    "    stream = client.text_generation(\n",
    "        text, \n",
    "        stream=True, \n",
    "        details=True, \n",
    "        max_new_tokens=3000,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    result = \"\"\n",
    "    for r in stream:\n",
    "        result += r.token.text\n",
    "        return re.sub(r'```(?:python)?\\n?|\\n?```', '', result.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b2813f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_code(python, model):\n",
    "    \"\"\"Generate documentation and comments for the provided Python code\"\"\"\n",
    "    try:\n",
    "        result = stream_with_hf_api(model, messages_for_comment(python, model))\n",
    "        for stream_so_far in result:\n",
    "            yield stream_so_far\n",
    "    except Exception as e:\n",
    "        yield f\"Error generating comments: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6133c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_test(python, model):\n",
    "    \"\"\"Generate unit tests for the provided Python code\"\"\"\n",
    "    try:\n",
    "        result = stream_with_hf_api(model, messages_for_unit_test(python, model))\n",
    "        for stream_so_far in result:\n",
    "            yield stream_so_far\n",
    "    except Exception as e:\n",
    "        yield f\"Error generating unit tests: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f384a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_code = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25dc094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".container {max-width: 1200px; margin: auto;}\n",
    ".model-selection {background-color: #f3f4f6; padding: 10px; border-radius: 8px;}\n",
    ".code-box {font-family: monospace; border: 1px solid #e5e7eb;}\n",
    ".button-primary {background-color: #3b82f6 !important;}\n",
    ".button-secondary {background-color: #10b981 !important;}\n",
    "\"\"\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=css, title=\"Python Code Documentation and Testing Generator\") as ui:\n",
    "    gr.Markdown(\"# Python Code Documentation and Testing Generator\", elem_classes=[\"container\"])\n",
    "    gr.Markdown(\"This tool generates comprehensive docstrings, comments, and unit tests for your Python code using Hugging Face models.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            python_input = gr.Textbox(\n",
    "                label=\"Python Code\", \n",
    "                value=sample_code, \n",
    "                lines=15,\n",
    "                elem_classes=[\"code-box\"]\n",
    "            )\n",
    "        \n",
    "        with gr.Column():\n",
    "            result_output = gr.Textbox(\n",
    "                label=\"Generated Output\", \n",
    "                lines=15,\n",
    "                elem_classes=[\"code-box\"]\n",
    "            )\n",
    "    \n",
    "    with gr.Row(elem_classes=[\"model-selection\"]):\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=list(MODEL_NAMES.keys()),\n",
    "            value=\"Mixtral\",\n",
    "            label=\"Select Model\"\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        comment_button = gr.Button(\"Generate Documentation\", elem_classes=[\"button-primary\"])\n",
    "        unit_test_button = gr.Button(\"Generate Unit Tests\", elem_classes=[\"button-secondary\"])\n",
    "    \n",
    "    comment_button.click(\n",
    "        comment_code, \n",
    "        inputs=[python_input, model_dropdown], \n",
    "        outputs=[result_output]\n",
    "    )\n",
    "    \n",
    "    unit_test_button.click(\n",
    "        get_unit_test, \n",
    "        inputs=[python_input, model_dropdown], \n",
    "        outputs=[result_output]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f56c5586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6966d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
